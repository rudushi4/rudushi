# Rudushi Model Configuration
# Fine-tuning configuration for creating Rudushi AI Assistant

model:
  name: "Rudushi"
  base_model: "tinyllama-1.1b"
  model_id: "megharudushi/Rudushi"
  description: "Rudushi - A lightweight language model optimized for mobile devices"

architecture:
  type: "CausalLM"
  parameters: "1.1B"
  vocab_size: 32000
  max_position_embeddings: 2048
  num_attention_heads: 32
  num_key_value_heads: 32
  hidden_size: 2048
  intermediate_size: 5632
  num_hidden_layers: 22

training:
  method: "LoRA"
  lora:
    rank: 16
    alpha: 16
    dropout: 0.0
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  hyperparameters:
    learning_rate: 2e-4
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    gradient_accumulation_steps: 4
    warmup_steps: 5
    max_steps: 100  # Increase for full training
    lr_scheduler_type: "linear"
    weight_decay: 0.01
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    seed: 3407

  optimization:
    fp16: false
    bf16: true
    dataloader_pin_memory: false
    remove_unused_columns: false
    gradient_checkpointing: true
    optim: "adamw_8bit"

dataset:
  name: "Alpaca"
  path: "tatsu-lab/alpaca"
  split: "train"
  size: 52000
  fields:
    instruction: "instruction"
    input: "input"
    output: "output"

prompt_template: |
  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

  ### Instruction:
  {instruction}

  ### Input:
  {input}

  ### Response:
  {output}

inference:
  quantization: "Q4_K_M"
  context_length: 2048
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1
  max_new_tokens: 512

performance:
  model_size_mb: 550
  ram_usage_mb: 500
  tokens_per_second: 5  # Mobile CPU
  first_token_latency_sec: 3
  context_window: 2048

metadata:
  license: "Apache 2.0"
  language: ["en"]
  tags:
    - "tinyllama"
    - "lora"
    - "4bit"
    - "mobile"
    - "termux"
    - "alpaca"
    - "chat"
    - "instruction-following"
    - "pytorch"
    - "text-generation"

  datasets_used:
    - "Alpaca (tatsu-lab/alpaca)"

  training_time: "Varies (1-4 hours on T4 GPU)"

  framework:
    - "PyTorch"
    - "Transformers"
    - "Unsloth"
    - "TRL"
    - "llama.cpp"

model_card:
  title: "Rudushi TinyLlama"
  short_description: "A 1.1B parameter language model fine-tuned for mobile AI applications"
  full_description: |
    Rudushi is a lightweight language model based on TinyLlama-1.1B, specifically optimized
    for mobile device inference using Termux on Android. The model has been fine-tuned on
    the Alpaca dataset using LoRA (Low-Rank Adaptation) for efficient training.

    Key features:
    - 1.1B parameters optimized for mobile
    - 4-bit quantization for efficient inference
    - 2048 token context window
    - Runs smoothly on Android with 4GB+ RAM
    - Suitable for chat, Q&A, and basic reasoning tasks

  usage: |
    ```python
    from transformers import AutoTokenizer, AutoModelForCausalLM

    model = AutoModelForCausalLM.from_pretrained("megharudushi/Rudushi")
    tokenizer = AutoTokenizer.from_pretrained("megharudushi/Rudushi")

    prompt = """Below is an instruction that describes a task, paired with an input.
    Write a response that appropriately completes the request.

    ### Instruction:
    Write a Python function to reverse a string.

    ### Response:
    """

    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=128)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    ```

  limitations: |
    - Knowledge cutoff: April 2024
    - Primarily English language support
    - May produce incorrect or biased responses
    - Not suitable for highly specialized technical tasks
    - Cannot browse the internet or access real-time information

  citation: |
    ```bibtex
    @misc{rudushi-tinyllama,
      title={Rudushi TinyLlama},
      author={Rudushi},
      year={2025},
      url={https://huggingface.co/megharudushi/Rudushi}
    }
    ```
