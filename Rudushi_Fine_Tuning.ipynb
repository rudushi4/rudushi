{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YOUR_USERNAME/rudushi/blob/main/Rudushi_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ü§ñ Rudushi Fine-Tuning Notebook\n",
        "\n",
        "**Create your custom Rudushi AI assistant!**\n",
        "\n",
        "This notebook will fine-tune TinyLlama-1.1B on the Alpaca dataset to create your custom Rudushi model.\n",
        "\n",
        "## ‚è±Ô∏è Estimated Time: 30-90 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes datasets transformers huggingface_hub safetensors\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "import os\n",
        "\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Load Base Model (TinyLlama-1.1B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"unsloth/tinyllama-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "DTYPE = None\n",
        "LOAD_IN_4BIT = True\n",
        "\n",
        "print(\"üì¶ Loading TinyLlama-1.1B model...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"   Parameters: {model.config.num_parameters / 1e6:.1f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Add LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lora"
      },
      "outputs": [],
      "source": [
        "# LoRA Configuration\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters added!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Load and Prepare Dataset (Alpaca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset"
      },
      "outputs": [],
      "source": [
        "# Alpaca prompt template\n",
        "ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = ALPACA_PROMPT.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"üìö Loading Alpaca dataset...\")\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Split into train and eval\n",
        "dataset_dict = dataset.train_test_split(test_size=0.05)\n",
        "train_dataset = dataset_dict[\"train\"]\n",
        "eval_dataset = dataset_dict[\"test\"]\n",
        "\n",
        "print(f\"‚úÖ Dataset prepared!\")\n",
        "print(f\"   Training: {len(train_dataset)} examples\")\n",
        "print(f\"   Evaluation: {len(eval_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_args"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=100,  # Adjust for more training\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        save_steps=50,\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Start Fine-Tuning! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting fine-tuning...\")\n",
        "print(\"‚è±Ô∏è  This will take 30-90 minutes depending on max_steps\")\n",
        "print(\"üìä Watch the loss decrease over time!\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Fine-tuning complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Save Your Rudushi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"fine_tuned_rudushi\"\n",
        "\n",
        "print(f\"üíæ Saving model to {OUTPUT_DIR}...\")\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "print(f\"üìÅ Location: /content/{OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Test Your Rudushi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test"
      },
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_prompt = ALPACA_PROMPT.format(\n",
        "    \"Write a Python function to reverse a string.\",\n",
        "    \"\",\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(\"üß™ Test output:\")\n",
        "print(response)\n",
        "print(\"\\n‚úÖ Test complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "## Step 10: Download Your Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"üì¶ Creating model archive...\")\n",
        "!zip -r rudushi_model.zip fine_tuned_rudushi/\n",
        "\n",
        "print(\"\\n‚¨áÔ∏è  Download your model:\")\n",
        "files.download(\"rudushi_model.zip\")\n",
        "\n",
        "print(\"\\n‚úÖ Model ready for download!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## üéâ Next Steps: Upload to Hugging Face\n",
        "\n",
        "1. **Upload the model to your device:**\n",
        "   - Transfer the downloaded `rudushi_model.zip` to your Android device\n",
        "   - Extract it in `/data/data/com.termux/files/home/rudushi/`\n",
        "\n",
        "2. **Convert to GGUF format:**\n",
        "   ```bash\n",
        "   cd /data/data/com.termux/files/home/rudushi\n",
        "   python3 02_convert_to_gguf.py\n",
        "   ```\n",
        "\n",
        "3. **Upload to Hugging Face:**\n",
        "   ```bash\n",
        "   python3 03_upload_to_hf.py --token YOUR_HF_TOKEN\n",
        "   ```\n",
        "\n",
        "4. **Deploy in Termux:**\n",
        "   ```bash\n",
        "   ./rudushi\n",
        "   # Then select: Chat with Rudushi\n",
        "   ```\n",
        "\n",
        "## üìö Your Custom Rudushi Model\n",
        "\n",
        "**Model Details:**\n",
        "- Base: TinyLlama-1.1B\n",
        "- Fine-tuned on: Alpaca dataset (52K instructions)\n",
        "- Method: LoRA (Rank 16)\n",
        "- Steps: 100 (adjustable)\n",
        "- Upload to: megharudushi/Rudushi\n",
        "\n",
        "Your Rudushi is ready to chat! ü§ñ‚ú®"
      ]
    }
  ]
}
